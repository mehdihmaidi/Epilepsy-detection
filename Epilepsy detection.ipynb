{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>4096</th>\n",
       "      <th>4097</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-56.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>-140.0</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>-114.0</td>\n",
       "      <td>-115.0</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>-114.0</td>\n",
       "      <td>-138.0</td>\n",
       "      <td>-159.0</td>\n",
       "      <td>-172.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-173.0</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-31.0</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-40.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>-88.0</td>\n",
       "      <td>-89.0</td>\n",
       "      <td>-81.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>187.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-147.0</td>\n",
       "      <td>-368.0</td>\n",
       "      <td>-550.0</td>\n",
       "      <td>-657.0</td>\n",
       "      <td>-665.0</td>\n",
       "      <td>-581.0</td>\n",
       "      <td>-442.0</td>\n",
       "      <td>-290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>510.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>763.0</td>\n",
       "      <td>703.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>-537.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-438.0</td>\n",
       "      <td>-561.0</td>\n",
       "      <td>-622.0</td>\n",
       "      <td>-581.0</td>\n",
       "      <td>-460.0</td>\n",
       "      <td>-295.0</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>443.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>-118.0</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-476.0</td>\n",
       "      <td>-518.0</td>\n",
       "      <td>-521.0</td>\n",
       "      <td>-362.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-261.0</td>\n",
       "      <td>-248.0</td>\n",
       "      <td>-147.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>23.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>-123.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>-231.0</td>\n",
       "      <td>-272.0</td>\n",
       "      <td>-272.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-221.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1      2      3      4      5      6      7      8      9     10  \\\n",
       "0     12.0   22.0   35.0   45.0   69.0   74.0   79.0   78.0   66.0   43.0   \n",
       "1    -56.0  -50.0  -64.0  -91.0 -135.0 -140.0 -134.0 -114.0 -115.0 -126.0   \n",
       "2    -37.0  -22.0  -17.0  -24.0  -31.0  -20.0   -5.0   14.0   31.0   31.0   \n",
       "3    -31.0  -43.0  -39.0  -39.0   -9.0   -5.0   18.0    7.0  -12.0  -42.0   \n",
       "4     14.0   26.0   32.0   25.0   16.0    8.0    8.0   12.0   11.0   19.0   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "495  -40.0  -58.0  -75.0  -88.0  -89.0  -81.0  -67.0  -52.0  -28.0   14.0   \n",
       "496  187.0   44.0 -147.0 -368.0 -550.0 -657.0 -665.0 -581.0 -442.0 -290.0   \n",
       "497 -438.0 -561.0 -622.0 -581.0 -460.0 -295.0 -164.0  -70.0    3.0   66.0   \n",
       "498 -476.0 -518.0 -521.0 -362.0  -68.0  175.0  289.0  184.0   15.0 -130.0   \n",
       "499   23.0  144.0  228.0  260.0  255.0  218.0  178.0  126.0   60.0    6.0   \n",
       "\n",
       "     ...   4089   4090   4091   4092   4093   4094   4095   4096   4097  label  \n",
       "0    ...  -28.0  -21.0  -14.0  -14.0  -25.0  -28.0  -11.0    8.0   77.0      0  \n",
       "1    ...  -82.0 -114.0 -138.0 -159.0 -172.0 -180.0 -173.0 -162.0  -82.0      0  \n",
       "2    ...  -52.0  -23.0  -14.0   -5.0   -3.0    7.0    3.0    4.0   82.0      0  \n",
       "3    ...  -32.0  -40.0  -23.0   -1.0   11.0   12.0   -6.0   10.0   33.0      0  \n",
       "4    ...  -19.0  -29.0  -35.0  -51.0  -55.0  -58.0  -32.0   -6.0  -17.0      0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "495  ...   32.0   32.0   18.0    6.0   -3.0  -10.0  -13.0  -16.0 -151.0      1  \n",
       "496  ...  510.0  562.0  607.0  667.0  748.0  763.0  703.0  446.0 -537.0      1  \n",
       "497  ...  443.0  399.0  319.0  196.0   40.0  -47.0 -118.0 -163.0  -56.0      1  \n",
       "498  ... -261.0 -248.0 -147.0   36.0  224.0  299.0  246.0  556.0  276.0      1  \n",
       "499  ... -127.0 -123.0 -152.0 -231.0 -272.0 -272.0 -155.0    6.0 -221.0      1  \n",
       "\n",
       "[500 rows x 4098 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/msi/dataset/pure_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SHAPES :  (500, 4098)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 4098 entries, 1 to label\n",
      "dtypes: float64(4097), int64(1)\n",
      "memory usage: 15.6 MB\n"
     ]
    }
   ],
   "source": [
    "print (\"DATA SHAPES : \",df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>4089</th>\n",
       "      <th>4090</th>\n",
       "      <th>4091</th>\n",
       "      <th>4092</th>\n",
       "      <th>4093</th>\n",
       "      <th>4094</th>\n",
       "      <th>4095</th>\n",
       "      <th>4096</th>\n",
       "      <th>4097</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.718000</td>\n",
       "      <td>-9.802000</td>\n",
       "      <td>-16.094000</td>\n",
       "      <td>-18.820000</td>\n",
       "      <td>-16.662000</td>\n",
       "      <td>-12.124000</td>\n",
       "      <td>-6.510000</td>\n",
       "      <td>-2.142000</td>\n",
       "      <td>1.882000</td>\n",
       "      <td>4.438000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.05600</td>\n",
       "      <td>-2.632000</td>\n",
       "      <td>-1.928000</td>\n",
       "      <td>-2.03800</td>\n",
       "      <td>-1.184000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>1.132000</td>\n",
       "      <td>-0.770000</td>\n",
       "      <td>-18.544000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>145.274622</td>\n",
       "      <td>163.176469</td>\n",
       "      <td>188.246611</td>\n",
       "      <td>201.245888</td>\n",
       "      <td>188.973686</td>\n",
       "      <td>165.080719</td>\n",
       "      <td>153.637922</td>\n",
       "      <td>155.370054</td>\n",
       "      <td>155.850617</td>\n",
       "      <td>155.882831</td>\n",
       "      <td>...</td>\n",
       "      <td>172.97619</td>\n",
       "      <td>166.175453</td>\n",
       "      <td>167.097438</td>\n",
       "      <td>177.47457</td>\n",
       "      <td>181.666176</td>\n",
       "      <td>173.855683</td>\n",
       "      <td>148.916496</td>\n",
       "      <td>119.354128</td>\n",
       "      <td>216.793244</td>\n",
       "      <td>0.400401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-985.000000</td>\n",
       "      <td>-1221.000000</td>\n",
       "      <td>-1406.000000</td>\n",
       "      <td>-1395.000000</td>\n",
       "      <td>-1291.000000</td>\n",
       "      <td>-880.000000</td>\n",
       "      <td>-998.000000</td>\n",
       "      <td>-1156.000000</td>\n",
       "      <td>-1009.000000</td>\n",
       "      <td>-665.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1224.00000</td>\n",
       "      <td>-1094.000000</td>\n",
       "      <td>-1400.000000</td>\n",
       "      <td>-1697.00000</td>\n",
       "      <td>-1547.000000</td>\n",
       "      <td>-1120.000000</td>\n",
       "      <td>-1073.000000</td>\n",
       "      <td>-734.000000</td>\n",
       "      <td>-1852.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-48.250000</td>\n",
       "      <td>-54.000000</td>\n",
       "      <td>-52.000000</td>\n",
       "      <td>-52.250000</td>\n",
       "      <td>-53.000000</td>\n",
       "      <td>-57.250000</td>\n",
       "      <td>-55.000000</td>\n",
       "      <td>-56.000000</td>\n",
       "      <td>-58.250000</td>\n",
       "      <td>-57.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.25000</td>\n",
       "      <td>-47.000000</td>\n",
       "      <td>-48.250000</td>\n",
       "      <td>-51.00000</td>\n",
       "      <td>-56.250000</td>\n",
       "      <td>-56.250000</td>\n",
       "      <td>-48.250000</td>\n",
       "      <td>-48.000000</td>\n",
       "      <td>-54.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>-8.500000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.50000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-9.500000</td>\n",
       "      <td>-6.50000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>37.250000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.250000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>32.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>41.25000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>800.000000</td>\n",
       "      <td>839.000000</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>876.000000</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>928.000000</td>\n",
       "      <td>973.000000</td>\n",
       "      <td>1045.000000</td>\n",
       "      <td>1381.000000</td>\n",
       "      <td>1502.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>911.00000</td>\n",
       "      <td>914.000000</td>\n",
       "      <td>919.000000</td>\n",
       "      <td>916.00000</td>\n",
       "      <td>829.000000</td>\n",
       "      <td>781.000000</td>\n",
       "      <td>703.000000</td>\n",
       "      <td>677.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1            2            3            4            5  \\\n",
       "count  500.000000   500.000000   500.000000   500.000000   500.000000   \n",
       "mean    -3.718000    -9.802000   -16.094000   -18.820000   -16.662000   \n",
       "std    145.274622   163.176469   188.246611   201.245888   188.973686   \n",
       "min   -985.000000 -1221.000000 -1406.000000 -1395.000000 -1291.000000   \n",
       "25%    -48.250000   -54.000000   -52.000000   -52.250000   -53.000000   \n",
       "50%     -8.000000    -8.000000    -7.000000    -9.000000    -8.500000   \n",
       "75%     36.000000    36.250000    37.250000    38.000000    41.000000   \n",
       "max    800.000000   839.000000   857.000000   876.000000   893.000000   \n",
       "\n",
       "                6           7            8            9           10  ...  \\\n",
       "count  500.000000  500.000000   500.000000   500.000000   500.000000  ...   \n",
       "mean   -12.124000   -6.510000    -2.142000     1.882000     4.438000  ...   \n",
       "std    165.080719  153.637922   155.370054   155.850617   155.882831  ...   \n",
       "min   -880.000000 -998.000000 -1156.000000 -1009.000000  -665.000000  ...   \n",
       "25%    -57.250000  -55.000000   -56.000000   -58.250000   -57.000000  ...   \n",
       "50%     -7.000000   -5.000000    -7.000000    -5.000000    -5.000000  ...   \n",
       "75%     40.000000   38.250000    36.000000    36.000000    32.250000  ...   \n",
       "max    928.000000  973.000000  1045.000000  1381.000000  1502.000000  ...   \n",
       "\n",
       "             4089         4090         4091        4092         4093  \\\n",
       "count   500.00000   500.000000   500.000000   500.00000   500.000000   \n",
       "mean     -4.05600    -2.632000    -1.928000    -2.03800    -1.184000   \n",
       "std     172.97619   166.175453   167.097438   177.47457   181.666176   \n",
       "min   -1224.00000 -1094.000000 -1400.000000 -1697.00000 -1547.000000   \n",
       "25%     -48.25000   -47.000000   -48.250000   -51.00000   -56.250000   \n",
       "50%      -9.50000    -7.000000    -9.500000    -6.50000    -7.000000   \n",
       "75%      32.00000    34.000000    39.000000    41.25000    42.000000   \n",
       "max     911.00000   914.000000   919.000000   916.00000   829.000000   \n",
       "\n",
       "              4094         4095        4096         4097       label  \n",
       "count   500.000000   500.000000  500.000000   500.000000  500.000000  \n",
       "mean      0.928000     1.132000   -0.770000   -18.544000    0.200000  \n",
       "std     173.855683   148.916496  119.354128   216.793244    0.400401  \n",
       "min   -1120.000000 -1073.000000 -734.000000 -1852.000000    0.000000  \n",
       "25%     -56.250000   -48.250000  -48.000000   -54.000000    0.000000  \n",
       "50%      -5.000000    -5.000000   -4.500000   -11.000000    0.000000  \n",
       "75%      46.000000    39.000000   31.250000    30.000000    0.000000  \n",
       "max     781.000000   703.000000  677.000000  1002.000000    1.000000  \n",
       "\n",
       "[8 rows x 4098 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25a077b88b0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].hist()\n",
    "#Not BaLANCED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "1025     0\n",
      "1375     0\n",
      "1374     0\n",
      "1373     0\n",
      "        ..\n",
      "2727     0\n",
      "2726     0\n",
      "2725     0\n",
      "2724     0\n",
      "1        0\n",
      "Length: 4098, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Statistical Test to determine whether input features are relevant to the outcome to be predicted.\n",
    "\n",
    "###P-value <= 0.05 significant result\n",
    "###P-value > 0.05 not significant result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4093</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>4094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>4095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>4096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>4097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4097 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Attribute  P-value\n",
       "0             1      0.0\n",
       "1             2      0.0\n",
       "2             3      0.0\n",
       "3             4      0.0\n",
       "4             5      0.0\n",
       "...         ...      ...\n",
       "4092       4093      0.0\n",
       "4093       4094      0.0\n",
       "4094       4095      0.0\n",
       "4095       4096      0.0\n",
       "4096       4097      0.0\n",
       "\n",
       "[4097 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "statistical_significance=[]\n",
    "for attr in range(1,4098):\n",
    "    data_count=pd.crosstab(df[str(attr)],df[\"label\"])\n",
    "    #print(data_count)\n",
    "    obs=data_count.values\n",
    "    #print(obs)\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "    statistical_significance.append([attr,round(p,4)])\n",
    "statistical_significance=pd.DataFrame(statistical_significance)\n",
    "statistical_significance.columns=[\"Attribute\",\"P-value\"]\n",
    "display(statistical_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.0002, 0.0001, 0.0003, 0.0005, 0.0007, 0.0006, 0.0004,\n",
       "       0.0008, 0.001 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistical_significance['P-value'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr=df.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:4097].values\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "x = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19508848,  0.27169238,  0.31744209, ..., -0.08155007,\n",
       "         0.07355241,  0.44115622],\n",
       "       [-0.24659351, -0.25474019, -0.35902492, ..., -1.17049755,\n",
       "        -1.35220687, -0.29299599],\n",
       "       [-0.07482829, -0.00481766, -0.02576543, ...,  0.01255651,\n",
       "         0.04000513,  0.46424276],\n",
       "       ...,\n",
       "       [-3.38130878, -3.22190562, -2.79629577, ..., -0.80079316,\n",
       "        -1.36059369, -0.17294594],\n",
       "       [-3.11752648, -2.68483805, -1.70698492, ...,  1.64597773,\n",
       "         4.66952937,  1.36000081],\n",
       "       [ 0.94349409,  1.29797003,  1.38685685, ..., -1.04950338,\n",
       "         0.05677877, -0.93480201]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,X.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             1065216   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,077,698\n",
      "Trainable params: 1,077,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,4096),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 3s 92ms/step - loss: 1.1017 - accuracy: 0.2592 - val_loss: 0.6987 - val_accuracy: 0.4700\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6750 - accuracy: 0.5674 - val_loss: 0.4229 - val_accuracy: 0.9100\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.3929 - accuracy: 0.9000 - val_loss: 0.3241 - val_accuracy: 0.9200\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.2888 - accuracy: 0.9318 - val_loss: 0.2804 - val_accuracy: 0.9300\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.2291 - accuracy: 0.9724 - val_loss: 0.2596 - val_accuracy: 0.9300\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1656 - accuracy: 0.9849 - val_loss: 0.2395 - val_accuracy: 0.9300\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1356 - accuracy: 0.9829 - val_loss: 0.2219 - val_accuracy: 0.9300\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1161 - accuracy: 0.9937 - val_loss: 0.2095 - val_accuracy: 0.9300\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0959 - accuracy: 0.9931 - val_loss: 0.1996 - val_accuracy: 0.9200\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0794 - accuracy: 0.9927 - val_loss: 0.1953 - val_accuracy: 0.9300\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0777 - accuracy: 0.9995 - val_loss: 0.1931 - val_accuracy: 0.9300\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0639 - accuracy: 0.9968 - val_loss: 0.1891 - val_accuracy: 0.9400\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0607 - accuracy: 0.9993 - val_loss: 0.1869 - val_accuracy: 0.9400\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.1862 - val_accuracy: 0.9400\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9400\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.1913 - val_accuracy: 0.9400\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.1940 - val_accuracy: 0.9400\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9500\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9400\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9500\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9500\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 1.0000 - val_loss: 0.1964 - val_accuracy: 0.9500\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.1981 - val_accuracy: 0.9500\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9500\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.2026 - val_accuracy: 0.9500\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.2051 - val_accuracy: 0.9500\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.2077 - val_accuracy: 0.9500\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.2091 - val_accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.2112 - val_accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.2133 - val_accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.2151 - val_accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9500\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9500\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9500\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9500\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.2237 - val_accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9500\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.2284 - val_accuracy: 0.9500\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.2302 - val_accuracy: 0.9500\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.2313 - val_accuracy: 0.9500\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.2332 - val_accuracy: 0.9500\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.2350 - val_accuracy: 0.9500\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.9500\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2383 - val_accuracy: 0.9500\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9500\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.2414 - val_accuracy: 0.9500\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.2423 - val_accuracy: 0.9500\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2439 - val_accuracy: 0.9500\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.2460 - val_accuracy: 0.9500\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 0.9500\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2505 - val_accuracy: 0.9500\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.2522 - val_accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.2539 - val_accuracy: 0.9500\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 0.9500\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9500\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.2580 - val_accuracy: 0.9500\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.2593 - val_accuracy: 0.9500\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2601 - val_accuracy: 0.9500\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9500\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2625 - val_accuracy: 0.9500\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.2646 - val_accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2664 - val_accuracy: 0.9500\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9500\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9500\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2716 - val_accuracy: 0.9500\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9500\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.2733 - val_accuracy: 0.9500\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2721 - val_accuracy: 0.9500\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 0.9500\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9500\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9500\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2759 - val_accuracy: 0.9500\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9500\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9500\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.9500\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2816 - val_accuracy: 0.9500\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9500\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2844 - val_accuracy: 0.9500\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9500\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2879 - val_accuracy: 0.9500\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9500\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.2899 - val_accuracy: 0.9500\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2913 - val_accuracy: 0.9500\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2925 - val_accuracy: 0.9500\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2940 - val_accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2952 - val_accuracy: 0.9500\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2962 - val_accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.2972 - val_accuracy: 0.9500\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9500\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2992 - val_accuracy: 0.9500\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3002 - val_accuracy: 0.9500\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.3012 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.3023 - val_accuracy: 0.9500\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3034 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3045 - val_accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.3058 - val_accuracy: 0.9500\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.3073 - val_accuracy: 0.9500\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9500\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "Training Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(expected_classes.shape)\n",
    "print(predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 17ms/step - loss: 0.7661 - val_loss: 0.8092\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7480 - val_loss: 0.8024\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7198 - val_loss: 0.8127\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7213 - val_loss: 0.7997\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7585 - val_loss: 0.7798\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7256 - val_loss: 0.7936\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7862 - val_loss: 0.8432\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7889 - val_loss: 0.8003\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7374 - val_loss: 0.8077\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7337 - val_loss: 0.8130\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7182 - val_loss: 0.8210\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7197 - val_loss: 0.8580\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7838 - val_loss: 0.8660\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7373 - val_loss: 0.8802\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7081 - val_loss: 0.8550\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7301 - val_loss: 0.8750\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7055 - val_loss: 0.8480\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7054 - val_loss: 0.8387\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6908 - val_loss: 0.8329\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6953 - val_loss: 0.8198\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7479 - val_loss: 0.8222\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6659 - val_loss: 0.8425\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7560 - val_loss: 0.8523\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7404 - val_loss: 0.8270\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7192 - val_loss: 0.8246\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7331 - val_loss: 0.8210\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6931 - val_loss: 0.8254\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7023 - val_loss: 0.8509\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7000 - val_loss: 0.8315\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7702 - val_loss: 0.8277\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6920 - val_loss: 0.8328\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7223 - val_loss: 0.8442\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7228 - val_loss: 0.8073\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6979 - val_loss: 0.8037\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7244 - val_loss: 0.8122\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7471 - val_loss: 0.8194\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6494 - val_loss: 0.8183\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7299 - val_loss: 0.8079\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7454 - val_loss: 0.8062\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7230 - val_loss: 0.8158\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7382 - val_loss: 0.8432\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7150 - val_loss: 0.8469\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7284 - val_loss: 0.8395\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7159 - val_loss: 0.8542\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6895 - val_loss: 0.7790\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6725 - val_loss: 0.8057\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6900 - val_loss: 0.7839\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6730 - val_loss: 0.7761\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6907 - val_loss: 0.7946\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6891 - val_loss: 0.7925\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6783 - val_loss: 0.8006\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6927 - val_loss: 0.8305\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7080 - val_loss: 0.7988\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6856 - val_loss: 0.7937\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6952 - val_loss: 0.7963\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.7051 - val_loss: 0.7968\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6927 - val_loss: 0.7989\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.7140 - val_loss: 0.7882\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7080 - val_loss: 0.8145\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7356 - val_loss: 0.8094\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7075 - val_loss: 0.8114\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6945 - val_loss: 0.8265\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7145 - val_loss: 0.8289\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6785 - val_loss: 0.8318\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6439 - val_loss: 0.8315\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6707 - val_loss: 0.8053\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7072 - val_loss: 0.8132\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6659 - val_loss: 0.7967\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7066 - val_loss: 0.8044\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6862 - val_loss: 0.8358\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7227 - val_loss: 0.8654\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6904 - val_loss: 0.7913\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7179 - val_loss: 0.8501\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6691 - val_loss: 0.8106\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6818 - val_loss: 0.8352\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 0.8300\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6727 - val_loss: 0.8387\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7438 - val_loss: 0.8038\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 0.7919\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7226 - val_loss: 0.7901\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6737 - val_loss: 0.7896\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7027 - val_loss: 0.7863\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6866 - val_loss: 0.7830\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7321 - val_loss: 0.7924\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7017 - val_loss: 0.7920\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6860 - val_loss: 0.7900\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7035 - val_loss: 0.7940\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7180 - val_loss: 0.7941\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6774 - val_loss: 0.7941\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6699 - val_loss: 0.7966\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7067 - val_loss: 0.7984\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6727 - val_loss: 0.8023\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6885 - val_loss: 0.7967\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6807 - val_loss: 0.7956\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6911 - val_loss: 0.7979\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7269 - val_loss: 0.7981\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6589 - val_loss: 0.7964\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6705 - val_loss: 0.7960\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6891 - val_loss: 0.7960\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7100 - val_loss: 0.7961\n"
     ]
    }
   ],
   "source": [
    "#simple rnn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 4096])\n",
    "])\n",
    "optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history = model.fit(x_train, y_train, epochs=100,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flask.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.save(\"flask.h5\")\n",
    "joblib.dump(x, \"flask.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "y_pred_logist=clf.predict(X[:, :])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbre de dec\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': np.arange(1,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9])})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=5)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = DecisionTreeClassifier(random_state=0, criterion='gini', max_depth=5 )\n",
    "final_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_score =  0.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('test_score = ', final_model.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_model, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[80  0]\n",
      " [ 9 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        80\n",
      "           1       1.00      0.55      0.71        20\n",
      "\n",
      "    accuracy                           0.91       100\n",
      "   macro avg       0.95      0.78      0.83       100\n",
      "weighted avg       0.92      0.91      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(x_train, y_train)\n",
    "y_pred = svclassifier.predict(x_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Erreur')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#KNN\n",
    "error = []\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(i)\n",
    "    knn_model = knn.fit(x_train, y_train)\n",
    "    pred_i = knn_model.predict(x_test)\n",
    "    error.append(np.mean(pred_i != y_test))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Taux Erreur pour les differentes valeurs de k')\n",
    "plt.xlabel('K ')\n",
    "plt.ylabel('Erreur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.89\n",
      "Accuracy of K-NN classifier on test set: 0.84\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(2)\n",
    "knn_model = knn.fit(x_train, y_train)\n",
    "y_pred_knn =knn_model.predict(x_test)\n",
    "\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(x_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve,KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold=StratifiedKFold(n_splits=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(kernel='linear'))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=0, criterion='gini', max_depth=5))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier(2))\n",
    "classifiers.append(LogisticRegression(random_state = random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = []\n",
    "for classifier in classifiers :\n",
    "    cv_results.append(cross_val_score(classifier, x_train, y = y_train, scoring = \"roc_auc\", cv =kfold , n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CrossValMeans  CrossValerrors           Algorithm\n",
      "0       0.713248        0.134988                 SVM\n",
      "1       0.818067        0.050094        DecisionTree\n",
      "2       0.995097        0.004678        RandomForest\n",
      "3       0.996310        0.001917          ExtraTrees\n",
      "4       0.995959        0.002705    GradientBoosting\n",
      "5       0.720915        0.029419         KNeighboors\n",
      "6       0.716542        0.191524  LogisticRegression\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_means = []\n",
    "cv_std   = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVM\",\"DecisionTree\",\n",
    "\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"KNeighboors\",\"LogisticRegression\"]})\n",
    "print(cv_res)\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
